Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Hannula2010,
abstract = {Results of several investigations indicate that eye movements can reveal memory for elements of previous experience. These effects of memory on eye movement behavior can emerge very rapidly, changing the efficiency and even the nature of visual processing without appealing to verbal reports and without requiring conscious recollection. This aspect of eye-movement based memory investigations is particularly useful when eye movement methods are used with special populations (e.g., young children, elderly individuals, and patients with severe amnesia), and also permits use of comparable paradigms in animals and humans, helping to bridge different memory literatures and permitting cross-species generalizations. Unique characteristics of eye movement methods have produced findings that challenge long-held views about the nature of memory, its organization in the brain, and its failures in special populations. Recently, eye movement methods have been successfully combined with neuroimaging techniques such as fMRI, single-unit recording, and MEG, permitting more sophisticated investigations of memory. Ultimately, combined use of eye-tracking with neuropsychological and neuroimaging methods promises to provide a more comprehensive account of brain-behavior relationships and adheres to the {\&}{\#}8220;converging evidence{\&}{\#}8221; approach to cognitive neuroscience.},
author = {Hannula, Deborah E. and Althoff, Robert R and Warren, David E and Riggs, Lily and Cohen, Neal J and Ryan, Jennifer D},
doi = {10.3389/fnhum.2010.00166},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hannula et al. - 2010 - Worth a glance using eye movements to investigate the cognitive neuroscience of memory.pdf:pdf},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {Amnesia,Eye Movements,Hippocampus,MEG,Memory,fMRI},
month = {oct},
pages = {166},
publisher = {Frontiers},
title = {{Worth a glance: using eye movements to investigate the cognitive neuroscience of memory}},
volume = {4},
year = {2010}
}
@article{Andersson2017,
abstract = {Almost all eye-movement researchers use algorithms to parse raw data and detect distinct types of eye movement events, such as fixations, saccades, and pursuit, and then base their results on these. Surprisingly, these algorithms are rarely evaluated. We evaluated the classifications of ten eye-movement event detection algorithms, on data from an SMI HiSpeed 1250 system, and compared them to manual ratings of two human experts. The evaluation focused on fixations, saccades, and post-saccadic oscillations. The evaluation used both event duration parameters, and sample-by-sample comparisons to rank the algorithms. The resulting event durations varied substantially as a function of what algorithm was used. This evaluation differed from previous evaluations by considering a relatively large set of algorithms, multiple events, and data from both static and dynamic stimuli. The main conclusion is that current detectors of only fixations and saccades work reasonably well for static stimuli, but barely better than chance for dynamic stimuli. Differing results across evaluation methods make it difficult to select one winner for fixation detection. For saccade detection, however, the algorithm by Larsson, Nystr{\"{o}}m and Stridh (IEEE Transaction on Biomedical Engineering, 60(9):2484-2493,2013) outperforms all algorithms in data from both static and dynamic stimuli. The data also show how improperly selected algorithms applied to dynamic data misestimate fixation and saccade properties.},
author = {Andersson, Richard and Larsson, Linnea and Holmqvist, Kenneth and Stridh, Martin and Nystr{\"{o}}m, Marcus},
doi = {10.3758/s13428-016-0738-9},
issn = {1554-3528},
journal = {Behavior Research Methods},
keywords = {Eye-tracking,Inter-rater reliability,Parsing},
month = {apr},
number = {2},
pages = {616--637},
pmid = {27193160},
title = {{One algorithm to rule them all? An evaluation and discussion of ten eye movement event-detection algorithms}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27193160 http://link.springer.com/10.3758/s13428-016-0738-9},
volume = {49},
year = {2017}
}
@article{Friedman2018,
author = {Friedman, Lee and Rigas, Ioannis and Abdulin, Evgeny and Komogortsev, Oleg V.},
doi = {10.3758/s13428-018-1050-7},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman et al. - 2018 - A novel evaluation of two related and two independent algorithms for eye movement classification during read(2).pdf:pdf},
issn = {1554-3528},
journal = {Behavior Research Methods},
month = {aug},
number = {4},
pages = {1374--1397},
publisher = {Springer US},
title = {{A novel evaluation of two related and two independent algorithms for eye movement classification during reading}},
url = {http://link.springer.com/10.3758/s13428-018-1050-7},
volume = {50},
year = {2018}
}
@article{Hanke2016,
abstract = {A {\textless}i{\textgreater}studyforrest{\textless}/i{\textgreater} extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation},
author = {Hanke, Michael and Adelh{\"{o}}fer, Nico and Kottke, Daniel and Iacovella, Vittorio and Sengupta, Ayan and Kaule, Falko R. and Nigbur, Roland and Waite, Alexander Q. and Baumgartner, Florian and Stadler, J{\"{o}}rg},
doi = {10.1038/sdata.2016.92},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hanke et al. - 2016 - A studyforrest extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation.pdf:pdf},
issn = {2052-4463},
journal = {Scientific Data},
keywords = {Attention,Cortex,Language,Neural encoding,Visual system},
month = {oct},
pages = {160092},
publisher = {Nature Publishing Group},
title = {{A studyforrest extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation}},
url = {http://www.nature.com/articles/sdata201692},
volume = {3},
year = {2016}
}
@inproceedings{Holmqvist2012,
address = {New York, New York, USA},
author = {Holmqvist, Kenneth and Nystr{\"{o}}m, Marcus and Mulvey, Fiona},
booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications - ETRA '12},
doi = {10.1145/2168556.2168563},
isbn = {9781450312219},
keywords = {accuracy,data quality,eye movements,eye tracker,latency,precision},
pages = {45},
publisher = {ACM Press},
title = {{Eye tracker data quality}},
url = {http://dl.acm.org/citation.cfm?doid=2168556.2168563},
year = {2012}
}
@article{HantaoLiu2011,
abstract = {Since the human visual system (HVS) is the ultimate assessor of image quality, current research on the design of objective image quality metrics tends to include an important feature of the HVS, namely, visual attention. Different metrics for image quality prediction have been extended with a computational model of visual attention, but the resulting gain in reliability of the metrics so far was variable. To better understand the basic added value of including visual attention in the design of objective metrics, we used measured data of visual attention. To this end, we performed two eye-tracking experiments: one with a free-looking task and one with a quality assessment task. In the first experiment, 20 observers looked freely to 29 unimpaired original images, yielding us so-called natural scene saliency (NSS). In the second experiment, 20 different observers assessed the quality of distorted versions of the original images. The resulting saliency maps showed some differences with the NSS, and therefore, we applied both types of saliency to four different objective metrics predicting the quality of JPEG compressed images. For both types of saliency the performance gain of the metrics improved, but to a larger extent when adding the NSS. As a consequence, we further integrated NSS in several state-of-the-art quality metrics, including three full-reference metrics and two no-reference metrics, and evaluated their prediction performance for a larger set of distortions. By doing so, we evaluated whether and to what extent the addition of NSS is beneficial to objective quality prediction in general terms. In addition, we address some practical issues in the design of an attention-based metric. The eye-tracking data are made available to the research community {\textless}citerefgrp{\textgreater}{\textless}citeref refid="ref1"/{\textgreater}{\textless}/citerefgrp{\textgreater}.},
author = {Liu, Hantao and Heynderickx, Ingrid},
doi = {10.1109/TCSVT.2011.2133770},
isbn = {1051-8215},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Eye tracking,image quality assessment,objective metric,saliency map,visual attention},
month = {jul},
number = {7},
pages = {971--982},
title = {{Visual attention in objective image quality assessment: Based on eye-tracking data}},
url = {http://ieeexplore.ieee.org/document/5740313/},
volume = {21},
year = {2011}
}
@article{Larsson2013,
abstract = {A novel algorithm for detection of saccades and postsaccadic oscillations in the presence of smooth pursuit movements is proposed. The method combines saccade detection in the acceleration domain with specialized on- and offset criteria for saccades and postsaccadic oscillations. The performance of the algorithm is evaluated by comparing the detection results to those of an existing velocity-based adaptive algorithm and a manually annotated database. The results show that there is a good agreement between the events detected by the proposed algorithm and those in the annotated database with Cohen's kappa around 0.8 for both a development and a test database. In conclusion, the proposed algorithm accurately detects saccades and postsaccadic oscillations as well as intervals of disturbances.},
author = {Larsson, Linnea and Nystrom, Marcus and Stridh, Martin},
doi = {10.1109/TBME.2013.2258918},
isbn = {1558-2531 (Electronic)
0018-9294 (Linking)},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {Eye-tracking,signal processing,smooth pursuit},
month = {sep},
number = {9},
pages = {2484--2493},
pmid = {23625350},
title = {{Detection of saccades and postsaccadic oscillations in the presence of smooth pursuit}},
url = {http://ieeexplore.ieee.org/document/6504734/},
volume = {60},
year = {2013}
}
@article{Larsson2013a,
abstract = {A novel algorithm for detection of saccades and postsaccadic oscillations in the presence of smooth pursuit movements is proposed. The method combines saccade detection in the acceleration domain with specialized on- and offset criteria for saccades and postsaccadic oscillations. The performance of the algorithm is evaluated by comparing the detection results to those of an existing velocity-based adaptive algorithm and a manually annotated database. The results show that there is a good agreement between the events detected by the proposed algorithm and those in the annotated database with Cohen's kappa around 0.8 for both a development and a test database. In conclusion, the proposed algorithm accurately detects saccades and postsaccadic oscillations as well as intervals of disturbances.},
author = {Larsson, Linnea and Nystrom, Marcus and Stridh, Martin},
doi = {10.1109/TBME.2013.2258918},
issn = {0018-9294},
journal = {IEEE Transactions on Biomedical Engineering},
month = {sep},
number = {9},
pages = {2484--2493},
pmid = {23625350},
title = {{Detection of Saccades and Postsaccadic Oscillations in the Presence of Smooth Pursuit}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23625350 http://ieeexplore.ieee.org/document/6504734/},
volume = {60},
year = {2013}
}
@article{Larsson2013b,
author = {Larsson, Linnea and Nystrom, Marcus and Stridh, Martin},
doi = {10.1109/TBME.2013.2258918},
issn = {0018-9294},
journal = {IEEE Transactions on Biomedical Engineering},
month = {sep},
number = {9},
pages = {2484--2493},
title = {{Detection of Saccades and Postsaccadic Oscillations in the Presence of Smooth Pursuit}},
url = {http://ieeexplore.ieee.org/document/6504734/},
volume = {60},
year = {2013}
}
@article{Hanke2014,
abstract = {A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie},
author = {Hanke, Michael and Baumgartner, Florian J. and Ibe, Pierre and Kaule, Falko R. and Pollmann, Stefan and Speck, Oliver and Zinke, Wolf and Stadler, J{\"{o}}rg},
doi = {10.1038/sdata.2014.3},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hanke et al. - 2014 - A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie.pdf:pdf},
issn = {2052-4463},
journal = {Scientific Data},
keywords = {Auditory system,Functional magnetic resonance imaging,Language,Perception},
month = {may},
pages = {140003},
publisher = {Nature Publishing Group},
title = {{A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie}},
url = {http://www.nature.com/articles/sdata20143},
volume = {1},
year = {2014}
}
@article{Harris2014,
abstract = {Face-selective regions in the amygdala and posterior superior temporal sulcus (pSTS) are strongly implicated in the processing of transient facial signals, such as expression. Here, we measured neural responses in participants while they viewed dynamic changes in facial expression. Our aim was to explore how facial expression is represented in different face-selective regions. Short movies were generated by morphing between faces posing a neutral expression and a prototypical expression of a basic emotion (either anger, disgust, fear, happiness or sadness). These dynamic stimuli were presented in block design in the following four stimulus conditions: (1) same-expression change, same-identity, (2) same-expression change, different-identity, (3) different-expression change, same-identity, and (4) different-expression change, different-identity. So, within a same-expression change condition the movies would show the same change in expression whereas in the different-expression change conditions each movie would have a different change in expression. Facial identity remained constant during each movie but in the different identity conditions the facial identity varied between each movie in a block. The amygdala, but not the posterior STS, demonstrated a greater response to blocks in which each movie morphed from neutral to a different emotion category compared to blocks in which each movie morphed to the same emotion category. Neural adaptation in the amygdala was not affected by changes in facial identity. These results are consistent with a role of the amygdala in category-based representation of facial expressions of emotion.},
author = {Harris, Richard J and Young, Andrew W and Andrews, Timothy J},
doi = {10.1016/j.neuropsychologia.2014.01.005},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harris, Young, Andrews - 2014 - Dynamic stimuli demonstrate a categorical representation of facial expression in the amygdala.pdf:pdf},
issn = {1873-3514},
journal = {Neuropsychologia},
keywords = {Emotion,Expression,Face,fMRI},
month = {apr},
number = {100},
pages = {47--52},
pmid = {24447769},
publisher = {Elsevier},
title = {{Dynamic stimuli demonstrate a categorical representation of facial expression in the amygdala.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24447769 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3988993},
volume = {56},
year = {2014}
}
@article{Toiviainen2014,
abstract = {We investigated neural correlates of musical feature processing with a decoding approach. To this end, we used a method that combines computational extraction of musical features with regularized multiple regression (LASSO). Optimal model parameters were determined by maximizing the decoding accuracy using a leave-one-out cross-validation scheme. The method was applied to functional magnetic resonance imaging (fMRI) data that were collected using a naturalistic paradigm, in which participants' brain responses were recorded while they were continuously listening to pieces of real music. The dependent variables comprised musical feature time series that were computationally extracted from the stimulus. We expected timbral features to obtain a higher prediction accuracy than rhythmic and tonal ones. Moreover, we expected the areas significantly contributing to the decoding models to be consistent with areas of significant activation observed in previous research using a naturalistic paradigm with fMRI. Of the six musical features considered, five could be significantly predicted for the majority of participants. The areas significantly contributing to the optimal decoding models agreed to a great extent with results obtained in previous studies. In particular, areas in the superior temporal gyrus, Heschl's gyrus, Rolandic operculum, and cerebellum contributed to the decoding of timbral features. For the decoding of the rhythmic feature, we found the bilateral superior temporal gyrus, right Heschl's gyrus, and hippocampus to contribute most. The tonal feature, however, could not be significantly predicted, suggesting a higher inter-participant variability in its neural processing. A subsequent classification experiment revealed that segments of the stimulus could be classified from the fMRI data with significant accuracy. The present findings provide compelling evidence for the involvement of the auditory cortex, the cerebellum and the hippocampus in the processing of musical features during continuous listening to music.},
author = {Toiviainen, Petri and Alluri, Vinoo and Brattico, Elvira and Wallentin, Mikkel and Vuust, Peter},
doi = {10.1016/J.NEUROIMAGE.2013.11.017},
issn = {1053-8119},
journal = {NeuroImage},
month = {mar},
pages = {170--180},
publisher = {Academic Press},
title = {{Capturing the musical brain with Lasso: Dynamic decoding of musical features from fMRI data}},
url = {https://www.sciencedirect.com/science/article/pii/S1053811913011099?via{\%}3Dihub},
volume = {88},
year = {2014}
}
@article{Holsanova2006,
abstract = {The aim of this article is to compare general assumptions about newspaper reading with eye-tracking data from readers' actual interaction with a newspaper. First, we extract assumptions about the way people read newspapers from socio-semiotic research. Second, we apply these assumptions by analysing a newspaper spread; this is done without any previous knowledge of actual reading behaviour. Finally, we use eye-tracking to empirically examine so-called entry points and reading paths. Eye movement data on reading newspaper spreads are analysed in three different ways: the time sequence in which different areas attract attention is calculated in order to determine reading priorities; the amount of time spent on different areas is calculated in order to determine which areas have been read most; the depth of attention is calculated in order to determine how carefully those areas have been read. General assumptions extracted from the socio-semiotic framework are compared to the results of the actual behaviour of subjects reading the newspaper spread. The results show that the empirical data confirm some of the extracted assumptions. The reading paths of the five subjects participating in the eye-tracking tests suggest that there are three main categories of readers: editorial readers, overview readers and focused readers.},
author = {Holsanova, Jana and Rahm, Henrik and Holmqvist, Kenneth},
doi = {10.1177/1470357206061005},
issn = {1470-3572},
journal = {Visual Communication},
month = {feb},
number = {1},
pages = {65--93},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Entry points and reading paths on newspaper spreads: comparing a semiotic analysis with eye-tracking measurements}},
url = {http://journals.sagepub.com/doi/10.1177/1470357206061005},
volume = {5},
year = {2006}
}
@article{Gordon2006,
author = {Gordon, Peter C. and Hendrick, Randall and Johnson, Marcus and Lee, Yoonhyoung},
doi = {10.1037/0278-7393.32.6.1304},
issn = {1939-1285},
journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
number = {6},
pages = {1304--1321},
title = {{Similarity-based interference during language comprehension: Evidence from eye tracking during reading.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.32.6.1304},
volume = {32},
year = {2006}
}
@article{Tikka2012,
abstract = {We outline general theoretical and practical implications of what we promote as enactive cinema for the neuroscientific study of online socio-emotional interaction. In a real-time functional magnetic resonance imaging (rt-fMRI) setting, participants are immersed in cinematic experiences that simulate social situations. While viewing, their physiological reactions - including brain responses - are tracked, representing implicit and unconscious experiences of the on-going social situations. These reactions, in turn, are analysed in real-time and fed back to modify the cinematic sequences they are viewing while being scanned. Due to the engaging cinematic content, the proposed setting focuses on living-by in terms of shared psycho-physiological epiphenomena of experience rather than active coping in terms of goal-oriented motor actions. It constitutes a means to parametrically modify stimuli that depict social situations and their broader environmental contexts. As an alternative to studying the variation of brain responses as a function of a priori fixed stimuli, this method can be applied to survey the range of stimuli that evoke similar responses across participants at particular brain regions of interest.},
author = {Tikka, Pia and V{\"{a}}ljam{\"{a}}e, Aleksander and de Borst, Aline W. and Pugliese, Roberto and Ravaja, Niklas and Kaipainen, Mauri and Takala, Tapio},
doi = {10.3389/fnhum.2012.00298},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tikka et al. - 2012 - Enactive cinema paves way for understanding complex real-time social interaction in neuroimaging experiments.pdf:pdf},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {Brain-Computer-Interfaces,enactive cinema,generative storytelling,implicit interaction,real-time fMRI,social neuroscience,two-way feedback},
month = {nov},
pages = {298},
publisher = {Frontiers},
title = {{Enactive cinema paves way for understanding complex real-time social interaction in neuroimaging experiments}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2012.00298/abstract},
volume = {6},
year = {2012}
}
@techreport{Larsson2016,
author = {Larsson and Linn{\'{e}}a},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Larsson, Linn{\'{e}}a - 2016 - P O B o x 1 1 7 2 2 1 0 0 L u n d 4 6 4 6-2 2 2 0 0 0 0 Event Detection in Eye-Tracking Data for Use in Applic.pdf:pdf},
title = {{P O B o x 1 1 7 2 2 1 0 0 L u n d + 4 6 4 6-2 2 2 0 0 0 0 Event Detection in Eye-Tracking Data for Use in Applications with Dynamic Stimuli}},
url = {http://portal.research.lu.se/portal/files/6192499/8600514.pdf},
year = {2016}
}
@article{Zemblys2018,
author = {Zemblys, Raimondas and Niehorster, Diederick C. and Komogortsev, Oleg and Holmqvist, Kenneth},
doi = {10.3758/s13428-017-0860-3},
file = {:C$\backslash$:/Users/Asim H. Dar/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zemblys et al. - 2018 - Using machine learning to detect events in eye-tracking data.pdf:pdf},
issn = {1554-3528},
journal = {Behavior Research Methods},
month = {feb},
number = {1},
pages = {160--181},
publisher = {Springer US},
title = {{Using machine learning to detect events in eye-tracking data}},
url = {http://link.springer.com/10.3758/s13428-017-0860-3},
volume = {50},
year = {2018}
}

@article{Nystrom2010AnData,
    title = {{An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data}},
    year = {2010},
    journal = {Behavior Research Methods},
    author = {Nystr{\"{o}}m, Marcus and Holmqvist, Kenneth},
    number = {1},
    month = {2},
    pages = {188--204},
    volume = {42},
    publisher = {Springer-Verlag},
    url = {http://www.springerlink.com/index/10.3758/BRM.42.1.188},
    doi = {10.3758/BRM.42.1.188},
    issn = {1554-351X}
}
@Article{Stampe1993,
	author="Stampe, Dave M.",
	title="Heuristic filtering and reliable calibration methods for video-based pupil-tracking systems",
	journal="Behavior Research Methods, Instruments, {\&} Computers",
	year="1993",
	month="Jun",
	day="01",
	volume="25",
	number="2",
	pages="137--142",
	abstract="Methods for enhancing the accuracy of fixation and saccade detection and the reliability of calibration in video gaze-tracking systems are discussed. The unique aspects of the present approach include effective low-delay noise reduction prior to the detection of fixation changes, monitoring of gaze position in real time by the operator, identification of saccades as small as 0.5{\textdegree} while eliminating false fixations, and a quick, high-precision, semiautomated calibration procedure.",
	issn="1532-5970",
	doi="10.3758/BF03204486",
	url="https://doi.org/10.3758/BF03204486"
}

@article{dorr2010variability,
	title={Variability of eye movements when viewing dynamic natural scenes},
	author={Dorr, Michael and Martinetz, Thomas and Gegenfurtner, Karl R and Barth, Erhardt},
	journal={Journal of vision},
	volume={10},
	number={10},
	pages={28--28},
	year={2010},
	publisher={The Association for Research in Vision and Ophthalmology}
}
