%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
    %!PS-Adobe-3.0 EPSF-3.0
    %%BoundingBox: 19 19 221 221
    %%CreationDate: Mon Sep 29 1997
    %%Creator: programmed by hand (JK)
    %%EndComments
    gsave
    newpath
    20 20 moveto
    20 220 lineto
    220 220 lineto
    220 20 lineto
    closepath
    2 setlinewidth
    gsave
    .4 setgray fill
    grestore
    stroke
    grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
%\documentclass[smallextended]{svjour3}       % onecolumn (second format)
\documentclass[smallextended,twocolumn]{svjour3}          % twocolumn
%
\journalname{Behavior Research Methods}
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
%usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
%\usepackage[natbibapa]{apacite}
\usepackage{natbib}


%\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{units}
\usepackage[draft]{hyperref} %tmp added draft option for spilling refs
\usepackage{wrapfig}
\usepackage{todonotes}
\newcommand{\eg}{e.g., }
\newcommand{\remodnav}{REMoDNaV}
\newcommand{\fig}[1]{{Figure~\ref{fig:#1}}}
\newcommand{\tab}[1]{{Table~\ref{tab:#1}}}
\newcommand{\param}[1]{{\texttt{#1}}}

\begin{document}

\input{results_def.tex}

\onecolumn
\title{REMoDNaV: Robust Eye Movement Detection for Natural Viewing } %\\ (remodnav)

% \titlenote{The title should be detailed enough for someone to know whether
% the article would be of interest to them, but also concise. Please ensure the
% broadness and claims within the title are appropriate to the content of the
% article itself.}

\author{%
  Asim~H.~Dar\textsuperscript{*} \and
Adina~Wagner\textsuperscript{*} \and
Michael~Hanke\\
{\small \textsuperscript{*} Both authors have contributed equally}}

\institute{Asim~H.~Dar \at
ADDME \\
%Tel.: +123-45-678910\\
%Fax: +123-45-678910\\
%\email{fauthor@example.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
\and
Adina Wagner \at
Psychoinformatics lab, Institute of Neuroscience and Medicine (INM-7: Brain and Behaviour), Research Centre Jülich, Germany
\and
Michael Hanke \at
Psychoinformatics lab, Institute of Neuroscience and Medicine (INM-7: Brain and Behaviour), Research Centre Jülich and Institute of Systems Neuroscience
Heinrich Heine University Düsseldorf, Germany
\email{michael.hanke@gmail.com}
}


%\affil[1]{Psychoinformatics Lab, Institute of Psychology, Otto-von-Guericke University, Magdeburg, Germany}

\date{Received: date / Accepted: date}

\maketitle

% Please list all authors that played a significant role in the research
% involved in the article. Please provide full affiliation information
% (including full institutional address, ZIP code and e-mail address) for all
% authors, and identify who is/are the corresponding author(s).

\begin{abstract}

% Abstracts should be up to 300 words and provide a succinct summary of the
% article. Although the abstract should explain why the article might be
% interesting, care should be taken not to inappropriately over-emphasize the
% importance of the work described in the article. Citations should not be used
% in the abstract, and the use of abbreviations should be minimized. If you are
% writing a Research or Systematic Review article, please structure your
% abstract into Background, Methods, Results, and Conclusions.

% general intro
Tracking of eye movements is an established measurement for many types of
experimental paradigms.
% targeted intro
More complex and lengthier visual stimuli have made algorithmic approaches to
eye movement event detection the most pragmatic option.
% problem statement
A recent analysis revealed that many current algorithms are lackluster when it
comes to data from viewing dynamic stimuli such as video sequences.
% out contribution
Here we present an event detection algorithm---built on an existing
velocity-based approach---that is suitable for both static and dynamic
stimulation, and is capable of detecting saccades, post-saccadic
oscillations, fixations, and smooth pursuit events.
% results
We validated detection performance and robustness on three public datasets:
1)~manually annotated, trial-based gaze trajectories for viewing static images,
moving dots, and short video sequences, 2)~lab-quality gaze recordings for a
feature length movie, and 3)~gaze recordings acquired under suboptimal lighting
conditions inside the bore of a magnetic resonance imaging (MRI) scanner for
the same full-length movie.
%
We found that the proposed algorithm performs on par or better compared
to state-of-the-art alternatives for static stimulation. Moreover, it yields
eye movement events with biologically plausible characteristics on prolonged
recordings without a trial-structure. Lastly, algorithm performance is robust
on data acquired under suboptimal conditions that are exhibiting a temporally
varying noise level.
% general context
These result indicate that the proposed algorithm is a robust tool with
improved detection accuracy across a range of use cases.
% broader perspective
A cross-platform compatible implementation in the Python programming language
is available as free and open source software.

\keywords{%
eye tracking \and
adaptive detection algorithm \and
saccade detection algorithm \and
statistical saccade analysis \and
glissade detection \and
adaptive threshold algorithm \and
data preprocessing
}
\end{abstract}

% \todo[inline]{The scope of the article is a "Data Note" that describes new
% "derived" data generated from the raw eyetracking data released by the
% studyforrest project. I propose to produce two types of artifacts: 1.
% filtered/preprocessed eyetracking data, and 2. a list with detected saccades
% for each recording.}

% \todo[inline]{It would be good to also release fully preprocessed data. Apart
% from applying the chosen filter, it would also make sense to me to temporally
% down-sample the data. What would be a practical sampling rate that reduces
% the data size (and some noise), but does not negatively impact most potential
% analyses? 100 Hz? 200 Hz? Even in the latter case it would still be a 5x
% reduction in size.}

% \todo[inline, backgroundcolor = green]{250 Hz is the lower limit to detect
% most saccades (Kern 2000). However,the downsampled data did not reach the
% same accuracy level as the higher frequency data, probably because the
% algorithm was designed to work on higher frequencies (as stated by the
% authors). Therefore we left it at the 1000 Hz frequency}

\twocolumn
\section*{Introduction}\label{intro}

% \todo[inline]{\textit{make connection to studyforrest. studyforrest has
% eyetracking data. why is it necessary to have that preprocessed?}}

% The data used for this thesis originates from the open science project
% 'studyforrest'. It centers around two large data acquisition phases employing
% the movie 'Forrest Gump' as stimulus. \cite{Hanke.2014,Hanke.2016} The
% project provides a large variety of collections of data to enable fellow
% researchers to build upon existing knowledge and further extend the dataset.
% Along other measures, the eye gaze coordinates were being recorded during the
% original sessions. Contrarily to the standard automatic detection process we
% applied an adaptive algorithm to the eye movement data to provide a more
% precise computation of saccades and fixations.

A spreading theme in cognitive neuroscience is to use dynamic and natural
stimuli as opposed to isolated and distinct imagery \citep{real_world}. Using
dynamic stimuli promises to observe the nuances of cognition in a more natural
environment. Some interesting applications include the determination of neural
response to changes in facial expression \citep{Harris2014}, understanding
complex social interactions by using videos \citep{Tikka2012} and more
untouched themes such as the underlying processing of music
\citep{Toiviainen2014}. In such studies, an unobtrusive behavioral measurement
is required to quantify the relationship between stimulus and response.
Tracking the focus of participants' gaze is a suitable, well established
measure that has been successfully employed in a variety of studies ranging
from the understanding of visual attention \citep{HantaoLiu2011}, memory
\citep{Hannula2010} and language comprehension \citep{Gordon2006}.
%
Regardless of use case, the raw eye tracking data (position coordinates)
provided by eye tracking devices are rarely used ``as is". Instead, in order
to disentangle different cognitive, occulomotor, or perceptive states
associated with different types of eye movements, most research relies on the
classification of eye gaze data into distinct eye movement event categories
\citep{Schutz2011}. The most feasible approach for doing this lies in the
application of appropriate event detection algorithms.

However, a recent comparison of algorithms found that while many readily
available algorithms for eye movement classification performed well on data
from static stimulation or short trial-based acquisitions with simplified
moving stimuli, none worked particularly well on data from complex natural
dynamic stimulation, such as video clips, when compared to human coders
\citep{Andersson2017}.
%
And indeed, when we evaluated an algorithm by \citet{Nystrom2010AnData}, one of
the winners in the aforementioned comparison, on data from prolonged
stimulation (\unit[$\approx$15]{min}) with a feature film, we found the
average and median durations of labeled fixations to exceeded literature
reports \citep[\eg][]{holmqvist2011eye,dorr2010variability} by up to a factor
of two.  Additionally, and in particular for increasing levels of noise in the
data, the algorithm classified too few fixations, as remarked by
\citet{Friedman2018}, because it discarded potential fixation events that
contained artifacts such as blinks.
%
However, robust performance on noisy data is of particular relevance in the
context of ``natural stimulation'', as the ultimate natural stimulation is the
actual natural environment, and data acquired outdoors or with mobile
devices typically does not match the quality achieved in dedicated lab
setups.

Therefore our objective was to improve upon the available eye movement
detection and classification algorithms, and develop a tool that performs
robustly on data from dynamic natural stimulation, without sacrificing quality
of performance for static and simplified stimulation. Importantly, we aimed for
applicability to prolonged recordings that lack any kind of trial structure,
and exhibit periods of signal-loss and non-stationary noise levels.
% maybe work in \citep{Hooge2018} again
In addition to the event categories \textit{fixation}, \textit{saccade}, and
\textit{post-saccadic oscillation} (sometimes termed ``glissade"), the
algorithm had to support the detection of \textit{smooth pursuit} events, as
emphasized by \cite{Friedman2018}.  These are slow movements of the eye during
tracking of a moving target and are routinely evoked by moving visual objects
during dynamic stimulation \citep{carl1987pursuits}.  If this type of eye
movement is not properly detected and labeled, erroneous fixation and saccade
events (which smooth pursuits would be classified into instead) are introduced.
Contemporary algorithms rarely provide this functionality (but see e.g.
\cite{LARSSON2015145}; \cite{Komogortsev2013} as existing algorithms with
smooth pursuit detection).

Here we introduce \remodnav\ (robust eye movement detection for natural
viewing), a novel tool that aims to meet these objectives. It is built on the
aforementioned algorithm by \citet{Nystrom2010AnData} (subsequently labeled NH)
that employs an adaptive approach to velocity based eye movement event
detection and classification. \remodnav\ enhances NH with the use of robust
statistics, and a compartmentalization of prolonged time series into short,
more homogeneous segments with more uniform noise levels. Furthermore, it adds
support for pursuit event detection. We evaluated \remodnav\ on three different
dataset from conventional paradigms, and natural stimulation (high and low
quality), and relate its performance to the algorithm comparison by
\cite{Andersson2017}.


\section*{Methods}\label{methods}

% Methods (3 sections; our algo, comparison to current algos, application on
% studyforrest dataset)

%\ todo[inline]{\textit{Elaborate on how the algorithm works;For software tool
%papers, this section should address how the tool works and any relevant
%technical details required for implementation of the tool by other
%developers.}}

Like NH, \remodnav\ is a \textit{velocity-based} event detection algorithm.
Compared to \textit{dispersion-based} algorithms, these types of algorithms are
less susceptible to noise and spatio-temporal precision, and thus applicable to
a large range of sampling frequencies. Furthermore, any influence of
biologically implausible velocities and accelerations can be prevented with the
use of appropriate filters and thresholds \citep{holmqvist2011eye}.

The algorithm comprises two major steps: preprocessing and event detection.  A
general overview and pseudo-code are shown in \fig{alg}.  The following
sections detail individual analysis steps. For each step relevant algorithm
parameters are given in parenthesis. \tab{parameters} summarizes all
parameters, and lists their default values.

\begin{figure*}
  \includegraphics[width=1\textwidth]{img/remodnav_algorithm.pdf}
  \caption{\remodnav workflow. Optional steps and configurable parameters are bold.}
  \label{fig:alg}
\end{figure*}


\subsection*{Preprocessing}

The goal of data preprocessing is to compute a time series of eye movement
velocities on which the event detection algorithm can be executed, while
reducing non-movement-related noise in the data as much as possible. 

First, implausible spikes in the coordinate time series are removed with a
heuristic spike filter \citep{stampe1993} (\fig{alg}A, 1). This filter is
standard in many eye tracking toolboxes and often used for preprocessing
\citep[\eg][]{Nystrom2010AnData}.
%
Data samples around signal loss (\eg eye blinks) can be nulled in order to
remove spurious movement signals (\param{dilate\_nan}, \param{min\_
blink\_duration}; \fig{alg}A, 2).
%
Coordinate time series are temporally filtered in two different ways
(\fig{alg}A, 3). A relatively large median filter
(\param{median\_filter\_ length}) is used to emphasize long-distance saccades.
This type of filtered data is later used for a coarse segmentation of a time
series into shorter intervals between major saccades.
%
Separately, data are also smoothed with a Savitzky-Golay filter
(\param{savgol\_\{length,polyord\}}). All event detection beyond the
localization of major saccades for time series chunking is performed on this
type of filtered data.

After spike-removal and temporal filtering, movement velocities and
accelerations are computed (\fig{alg}A, 4-5). To
disregard biologically implausible measurements, a configurable maximum
velocity (\param{max\_vel}) is enforced -- samples exceeding this threshold
are replaced by this set value.

\begin{table*}[tbp]
  \caption{Algorithm parameters and their default values}
  \label{tab:parameters}
  \small
  \begin{tabular}{lp{85mm}l}
    \textbf{Name} & \textbf{Description} & \textbf{Value} \\
    & & \\
    \multicolumn{3}{l}{\textit{Preprocessing (in order of application during processing)}} \\
    \texttt{px2deg} &
    size of a single (square) pixel in degrees of visual angle &
    no default [\unit{deg/s}]\\
    \texttt{sampling\_rate} &
    temporal data sampling rate/frequency &
    no default [\unit{Hz}]\\
    \texttt{min\_blink\_duration} &
    missing data windows shorter than this duration will not be considered for \texttt{dilate\_nan}&
    \unit[0.02]{s}\\
    \texttt{dilate\_nan} &
    duration for which to replace data by missing data markers on either side of a
    signal-loss window &
    \unit[0.01]{s}\\
    \texttt{median\_filter\_length} &
    smoothing median-filter size (for initial data chunking only) &
    \unit[0.05]{s}\\
    \texttt{savgol\_length} &
    size of Savitzky-Golay filter for noise reduction&
    \unit[0.019]{s}\\
    \texttt{savgol\_polyord} &
    polynomial order of Savitzky-Golay filter for noise reduction&
    2\\
    \texttt{max\_vel} &
    maximum velocity threshold, will replace value with maximum, and issue
    warning if exceeded to inform about
    potentially inappropriate filter settings
    (default value based on \cite{holmqvist2011eye})&
    \unit[1000]{deg/s}\\

    \\\multicolumn{3}{l}{\textit{Event detection}} \\
    \texttt{min\_saccade\_duration} &
    minimum duration of a saccade event candidate &
    \unit[0.01]{s}\\
    \texttt{max\_pso\_duration} &
    maximum duration of a post-saccadic oscillation (glissade) candidate &
    \unit[0.04]{s}\\
    \texttt{min\_fixation\_duration} &
    minimum duration of a fixation event candidate &
    \unit[0.04]{s}\\
    \texttt{min\_pursuit\_duration} &
    minimum duration of a pursuit event candidate &
    \unit[0.04]{s}\\
    \texttt{min\_intersaccade\_duration} &
    no saccade detection is performed in windows shorter than twice this value, plus minimum saccade and PSO duration&
    \unit[0.04]{s}\\
    \texttt{noise\_factor} &
    adaptive saccade onset threshold velocity is the median absolute deviation of velocities in the window of interest, times this factor (peak velocity threshold is twice the onset velocity); increase for noisy data to reduce false positives \citep[equivalent: 3.0]{Nystrom2010AnData}&
    5\\
    \texttt{velthresh\_startvelocity} &
    start value for adaptive velocity threshold algorithm \citep{Nystrom2010AnData}, should
    be larger than any conceivable minimum saccade velocity &
    \unit[300]{deg/s}\\
    \texttt{max\_initial\_saccade\_freq} &
    maximum saccade frequency for initial detection of major saccades, initial data
    chunking is stopped if this frequency is reached (should be smaller than an expected
    (natural) saccade frequency in a particular context), default based on literature reports of a natural, free-viewing saccade frequency of \unit[$>$2.5]{Hz} (Otero-Millan, 2008; Hancock et al., 2012)\todo[inline]{add proper references}&
    \unit[2]{Hz}\\
    \texttt{saccade\_context\_window\_length} &
    size of a window centered on any velocity peak for adaptive determination of
    saccade velocity thresholds (for initial data chunking only) &
    \unit[1]{s}\\
    \texttt{lowpass\_cutoff\_freq} &
    cut-off frequency of a Butterworth low-pass filter applied to determine drift
    velocities in a pursuit event candidate &
    \unit[4]{Hz}\\
    \texttt{pursuit\_velthresh} &
    fixed drift velocity threshold to distinguish periods of pursuit from periods of fixation &
    \unit[2]{deg/s}\\
  \end{tabular}
\end{table*}



\subsection*{Event detection}

\subsubsection*{Saccade velocity threshold}

Except for a few modifications, \remodnav\ employs the adaptive saccade
detection algorithm proposed by \cite{Nystrom2010AnData}, where saccades
are initially located by thresholding the velocity time series by a critical
value. Starting from an initial velocity threshold (\param{velthres\_
startvelocity}, termed $PT_1$ in NH), the critical value is determined
adaptively by computing the variance of sub-threshold velocities ($V$), and
placing the new velocity threshold at:
%
\begin{equation}
  PT_n = \overline{V}_{n-1} + F \times \sqrt{{\sum(V_{n-1} -
\overline{V}_{n-1})^2} \over {N-1}}
\end{equation}
%
where $F$ determines how many standard deviations above the average
velocity the new threshold is located.  This procedure is repeated until it
stabilizes on a threshold velocity.
%
\begin{equation}
|PT_n - PT_{n-1}| < 1^\circ/sec
\end{equation}

\remodnav\ alters this algorithm by using robust statistics that are more
suitable for the non-normal distribution of velocities \citep{Friedman2018},
such that the new threshold is computed by:
%
\begin{equation}\label{eq:threshold}
PT_n = median({V}_{n-1}) + F \times MAD({V}_{n-1})
\end{equation}
%
where $MAD$ is the median absolute deviation, and $F$ is a
scalar parameter of the algorithm.

\subsection*{Time series chunking}

As the algorithm aims to be applicable to prolonged recordings without an
inherent trial structure and inhomogeneous noise levels, the time series needs
to be split into shorter chunks in order to prevent negative impact of sporadic
noise flares on the aforementioned adaptive velocity thresholding procedure.

\remodnav\ implements this chunking by determining a critical velocity on a
median-filtered (\param{median\_ filter\_length}) time series comprising the
full duration of a recording (\fig{alg}D). Due to potentially elevated noise
levels the resulting threshold tends to overestimate an optimal threshold.
Consequently, only periods of fastest eye movements will exceed this threshold.
All such periods of consecutive above-threshold velocities are weighted by the
sum of these velocities. Boundaries of time series chunks are determined by
selecting such events sequentially (starting with the largest sums), until a
maximum average frequency across the whole time series is reached
(\param{max\_initial\_saccade\_ freq}). The resulting chunks represent data
intervals between saccades of maximum magnitude in the respective data.


\subsection*{Detection of saccades and post-saccadic oscillations}

Detection of these event types is identical to the NH algorithm, only the data
context and metrics for determining the velocity thresholds differ.  For
saccades that also represent time series chunk boundaries (event label
\texttt{SACC}), a context of \unit[1]{s}
(\param{saccade\_context\_window\_ length}) centered on the peak velocity is
used by default, for any other saccade (event label \texttt{ISAC}) the entire
time series chunk represents that context (\fig{alg}E).

Peak velocity threshold and on/offset velocity threshold are then determined by
equation \ref{eq:threshold} with $F$ set to $2\times\mathtt{noise\_factor}$ and
\param{noise\_factor}, respectively. Starting from a velocity peak the
immediately preceding and the following velocity minima that do not exceed the
on/offset threshold are located and used as event boundaries. Qualifying events
are rejected, if they do not exceed a configurable minimum duration, or violate
the set saccade maximum proximity criterion (\param{min\_ saccade\_duration},
\param{min\_intersaccade\_duration}).

As in NH, post-saccadic oscillations are events that immediately follow a
saccade, where the velocity exceeds saccade velocity threshold within a short
time window (\param{max\_pso\_duration}). \remodnav\ distinguishes low-velocity
(event label \texttt{LPSO} for chunk boundary event, \texttt{ILPS} otherwise)
and high-velocity oscillations (event label \texttt{HPSO} or \texttt{IHPS}),
where the velocity exceeds the saccade onset or peak velocity threshold,
respectively.

\subsection*{Pursuit and fixation detection}

For all remaining, unlabeled time series segments that are longer than a
minimum duration (\param{min\_fixation\_ duration}), velocities are low-pass
filtered (Butterworth, \param{lowpass\_cutoff\_freq}). Any segments
exceeding a minimum velocity threshold (\param{pursuit\_velthresh}) are
classified as pursuit (event label \texttt{PURS}). Pursuit on/offset detection
uses the same approach as that for saccades: search for local minima preceding
and following the above threshold velocities.
%
Any remaining segment that does not qualify as a pursuit event is classified
as a fixation (event label \texttt{FIXA}).


\subsection*{Operation}\label{op}

\remodnav\ is free and open-source software, written in the Python language and
released under the terms of the MIT license. In addition to the Python standard
library it requires the Python packages
%
NumPy \citep{oliphant2006guide},
Matplotlib \citep{hunter2007matplotlib},
statsmodels \citep{seabold2010statsmodels},
and SciPy \citep{JOP+2001} as software dependencies.
Furthermore, DataLad \citep{HH+2013},
and Pandas \citep{mckinney2010data}
%
have to be available to run the test
battery. \remodnav\ itself, and all software dependencies are available on all
major operating systems.  There are no particular hardware requirements for
running the software other than sufficient memory to load and process the data.

Program outputs are BIDS-compliant \citep{gorgolewski2016brain}
tab-separated-value (TSV) text files. They contain a report on one detected
event per line, with onset and offset time, onset and offset coordinates,
amplitude, peak velocity, median velocity and average velocity.  Additionally,
a visualization of the detection results, together with a time course of
horizontal and vertical gaze position, and velocities is provided for
illustration and initial quality assessment (see \fig{remodnav} for an
example).

All available parameters (sorted by algorithm step) with their description and,
if applicable, default value, are listed in \tab{parameters}.  While
input required by a user is kept minimal (raw data, temporal sampling rate,
viewing distance and screen size), the number of configurable parameters is
purposefully large.
%


\section*{Validation analyses}\label{ana}

% \todo[inline]{three major types of comparison: with andersson human labeling,
% stats of forrest lab recording with andersson video data stats, forrest lab
% vs forrest mri stats. the goal is to show that we are similar to humans, as
% good (or better) as other algorithms (by comparison with scores in
% andersson2017), and proceduce "similar" results on a different movie dataset,
% and similar results across two different qualities of recordings with the
% same stimulus (lab vs MRI). No more, no less IMHO. This all translates to
% three use cases: trial-by-trial data (from anderson), good movie data without
% trial structure (forrest lab), bad movie data (forrest mri)}

% THIS SECTION  WILL BASICALLY SHOW THE INPUTS AND THE OUTPUTS(RESULTS
% BASICALLY)

% \todo[inline]{\textit{Testing and comparison --- explaining rationale of the
% compared algorithms ie they were winners in the anderson paper}}

We applied our algorithm to two datasets, each to fulfill different objectives.
First, in order to validate REMoDNaV, we used the manually annotated dataset
from the Humanities Lab, Lund University, used by
\cite{Andersson2017}\footnote{github.com/richardandersson/EyeMovementDetector\linebreak[0]Evaluation},
to see how our algorithm compared against the NH algorithm it is based on and
manual human annotation of eye gaze events. This was done to verify that
changes introduced in \remodnav\ do not decrease performance below that of the
NH algorithm, and to evaluate whether REMoDNaV indeed improves eye movement
detection during dynamic stimulation as intended, compared to ten contemporary
event detection algorithms. Secondly, we used our algorithm on the studyforrest
dataset \citep{Hanke2016} to see the results of dynamic stimuli under two
different noise conditions: A high-quality dataset from a laboratory setting,
and low-quality data from a simultaneous fMRI and eye gaze acquisition.  This
analysis aims to provide an estimate of the robustness of the algorithm results
and follows a simple reasoning: As the stimulus (the Hollywood movie: Forrest
Gump) is the same in both noise conditions, it should evoke similar gaze
characteristics regardless of the quality of its measurement. Should \remodnav\
extract similar eye event properties, then this is a good indicator of robust
performance even under high noise conditions.

\subsection*{Evaluation of the algorithm: comparing outputs against human
coders and a current algorithm}\label{ana_1}

The dataset provided by \cite{Andersson2017} consists of monocular eye gaze
data produced from viewing stimuli from three distinct categories -- images,
moving dots and videos. In addition to the eye tracking metadata (eye tracker
sampling rate (500hz), size of the screen, raw eye gaze data, distance from
screen) and eye gaze data (coordinates), each frame was also labeled by two
human observers to assign the event they thought it was. A total of six
different labels were used, namely, fixation, saccade, post-saccadic
oscillation, smooth pursuit, blink and undefined (a sample that did not fit any
other category). A minor labeling mistake reported in \cite{Zemblys2018} was
fixed prior to the computations.

To compare the REMoDNaV results to the human coders and the performance of the
NH algorithm, we applied \remodnav\ to the Humanities lab dataset and followed
the approach by \citet{Andersson2017}: For one, for each stimulus category, we
computed the proportion of misclassification per event type compared to each of
the human coders, and compared human coders against each other. A time point
classified by \remodnav\ counted as ``misclassified" if the associated event did
not correspond to the label the human coder assigned. We limited this analysis
to all time points that have been labeled as fixation, saccade, PSO, or pursuit
by any method, hence ignoring the rarely used NaN/blinks or ``undefined"
category. In order to provide a comparison to NH and the other algorithms in
\cite{Andersson2017}, this misclassification analysis was repeated excluding
samples labeled as pursuit (as none of the algorithms classed this category).
The results of this analysis are displayed in \tab{mclf}. For the
comparison performed by \citet{Andersson2017}, the NH algorithm disagreed with
human coders in 32\% of samples for images, in 93\% of samples for moving dots,
and in 70\% of samples for videos. In a direct comparison, i.e. if smooth
pursuit movements are not taken into account, our algorithm had a lower
misclassification rate. The highest disagreement across all stimuli categories
and coders is \maxmclf\%. Compared to all ten algorithms used in
\citet{Andersson2017}, \remodnav\ displays the lowest misclassification rates in
all categories.

When taking smooth pursuit movements into account, the misclassification ratio
increases, as would be expected with the introduction of a fourth eye movement
category. Importantly, despite an increase, the misclassification rate remains
comparably low, thus still exceeding the performance of all contemporary
algorithms in \citet{Andersson2017} in the dynamic categories dots and videos.
For a comparison between algorithm and human coders classification, \fig{conf}
displays the resulting confusion matrices with Jaccard indices
\citep[JI; ][]{jaccard1901etude} quantifying the similarity between
classification decisions. The JI is bound in range [0, 1] with higher values
indicating higher similarity. A value of 0.93 in the upper left cell of the
very first matrix in \fig{conf} for example indicates that 93\% of frames that
are labeled as a fixation by human coders RA and MN are the same. This index
allows to quantify the similarity of classifications independent from values in
other cells. While the highest similarity in labeling (evident from high values
along the diagonal) exists between human coders, the algorithm still performs
well. For the new event type smooth pursuit, detection is very consistent to
human labels especially in the moving dot category (JI of .79 and .75,
respectively), and still good in the video category (JI of .56 and .63,
respectively).  \todo[inline]{if we keep these values in here, we might want to
make Latex commands for them as well? }

\begin{table}[tbp]
  % table caption is above the table
  \caption{Confusion matrices for different stimulus categories}
  \label{tab:mclf}       % Give a unique label
  % For LaTeX tables use
  \begin{tabular}{llllllll}
    \textbf{Images}&&&&&&&\\
    \hline\noalign{\smallskip}
    Comp & MC & w/oP & Coder & Fix & Sac & PSO & SP \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    MN-RA & \imgMNRAMCLF & \imgMNRAMclfWOP & MN & \imgMNRAFIXref & \imgMNRASACref & \imgMNRAPSOref & \imgMNRASPref  \\
    --- & --- & --- & RA & \imgMNRAFIXcod & \imgMNRASACcod & \imgMNRAPSOcod & \imgMNRASPcod \\
    MN-AL & \imgMNALMCLF & \imgMNALMclfWOP & MN & \imgMNALFIXref & \imgMNALSACref & \imgMNALPSOref & \imgMNALSPref \\
    --- & --- & --- & AL & \imgMNALFIXcod & \imgMNALSACcod & \imgMNALPSOcod & \imgMNALSPcod \\
    RA-AL & \imgRAALMCLF & \imgRAALMclfWOP & RA & \imgRAALFIXref & \imgRAALSACref & \imgRAALPSOref & \imgRAALSPref \\
    ---& ---& ---& AL & \imgRAALFIXcod & \imgRAALSACcod & \imgRAALPSOcod & \imgRAALSPcod \\
    \noalign{\smallskip}
    \textbf{Dots}&&&&&&&\\
    \hline\noalign{\smallskip}
    Comp & MC & w/oP & Coder & Fix & Sac & PSO & SP \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    MN-RA & \dotsMNRAMCLF & \dotsMNRAMclfWOP & MN & \dotsMNRAFIXref & \dotsMNRASACref & \dotsMNRAPSOref & \dotsMNRASPref  \\
    --- & --- & --- & RA & \dotsMNRAFIXcod & \dotsMNRASACcod & \dotsMNRAPSOcod & \dotsMNRASPcod \\
    MN-AL & \dotsMNALMCLF & \dotsMNALMclfWOP & MN & \dotsMNALFIXref & \dotsMNALSACref & \dotsMNALPSOref & \dotsMNALSPref \\
    --- & --- & --- & AL & \dotsMNALFIXcod & \dotsMNALSACcod & \dotsMNALPSOcod & \dotsMNALSPcod\\
    RA-AL & \dotsRAALMCLF & \dotsRAALMclfWOP & RA & \dotsRAALFIXref & \dotsRAALSACref & \dotsRAALPSOref & \dotsRAALSPref \\
    ---& ---& ---& AL & \dotsRAALFIXcod & \dotsRAALSACcod & \dotsRAALPSOcod & \dotsRAALSPcod \\
    \noalign{\smallskip}
    \textbf{Videos}&&&&&&&\\
    \hline\noalign{\smallskip}
    Comp & MC & w/oP & Coder & Fix & Sac & PSO & SP \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    MN-RA & \videoMNRAMCLF & \videoMNRAMclfWOP & MN & \videoMNRAFIXref & \videoMNRASACref & \videoMNRAPSOref & \videoMNRASPref \\
    --- & --- & --- & RA & \videoMNRAFIXcod & \videoMNRASACcod & \videoMNRAPSOcod & \videoMNRASPcod \\
    MN-AL & \videoMNALMCLF & \videoMNALMclfWOP & MN & \videoMNALFIXref & \videoMNALSACref & \videoMNALPSOref & \videoMNALSPref \\
    --- & --- & --- & AL & \videoMNALFIXcod & \videoMNALSACcod & \videoMNALPSOcod & \videoMNALSPcod\\
    RA-AL & \videoRAALMCLF & \videoRAALMclfWOP & RA & \videoRAALFIXref & \videoRAALSACref & \videoRAALPSOref & \videoRAALSPref \\
    ---& ---& ---& AL & \videoRAALFIXcod & \videoRAALSACcod & \videoRAALPSOcod & \videoRAALSPcod \\
    \noalign{\smallskip}\hline
  \end{tabular}

  \textit{Note}: Proportion of samples in in the image category classified in
  disagreement within the human coders (MN-RA) or between \remodnav\ algorithm
  (AL) and one human coder. The MC (misclassification) column indicates the
  proportion of samples where the human coders disagreed with each other, or
  where the algorithm disagreed with a human coder. The w/oP (without pursuit)
  column is the same measure, but excludes all pursuit samples. The remaining
  columns display the percentage of labels used in misclassified samples.
  Within each row, percentages per Coder should add up tp 100\%, in rare cases
  rounding leads to 99 or 101\%, though.
  \todo[inline]{add explicit reference to tables in \citet{Andersson2017} that
  contain the analog results}

\end{table}

% Simple quick example of what I envision a confusion matrix graphic could look
% like.

\begin{figure*}
  % Use the relevant command to insert your figure file.
  % For example, with the graphicx package use
  % TODO make final figure and switch
  %\includegraphics[width=1\textwidth]{img/conf_drawing.eps}
  \includegraphics[width=1\textwidth]{img/confusion_MN_RA.pdf} \\
  \includegraphics[width=1\textwidth]{img/confusion_MN_AL.pdf} \\
  \includegraphics[width=1\textwidth]{img/confusion_RA_AL.pdf}
  % figure caption is below the figure

  \caption{Confusion patterns for pairwise eye movement classification
    comparison of both human rater \citep[MN and RA; ][]{Andersson2017} and the
    \remodnav\ algorithm (AL) for gaze recordings from stimulation with static
    images (left column), moving dots (middle column), and video clips (right
    column).  All matrices present gaze sample based Jaccard indices \citep[JI;
    ][]{jaccard1901etude}. Consequently, the diagonals depict the fraction of
    time points labeled congruently by both raters in relation to the number of
    timepoints assigned to a particular event category by any rater.}
  % Give a unique label
  \label{fig:conf}
\end{figure*}

%\begin{figure*}[!ht]
%\centering
%\includegraphics[width=1\textwidth]{Mainseqs_legends.pdf}
%\caption{\label{fig:Mainseqs}Overview of use cases to validate to anderson dataset. Three sets of stimuli, namely, images, dots, and videos which are processed by human observers and the algorithm. Sample A shows results that show a good match between the human and algorithmic results,sample B showing acceptable results, while sample C shows a poor match. In the case of the human observers the results of each observer --- represented by either blue or red --- are superimposed onto one another. Event detection done by the algorithm additionally split saccadic events into Segment defining saccades (SD-S) and PSOs to regular and High velocity PSOs (HV-PSO)  }

%\end{figure*}

Additionally, for each stimulus category (images, dots, videos), we determined
the mean and standard deviation of the event durations per event category (fixations,
saccades, PSOs, pursuits) and the number of detected events per event category.
Based on these results, following equation 2 in \citet{Andersson2017}, we computed
the root mean squared deviations (RMSD) for the full set of algorithms, human coders,
and REMoDNav. The RMSD is proposed by \cite{Andersson2017} as a single similarity
measure of distribution parameters to rank algorithms according to their similarity
to human coders in three dimensions. As \citet{Andersson2017} point out, this measure
ensures that we do not only evaluate performance on a sample-by-sample classification
similarity, but also evaluate whether an algorithm detects similar event counts and
duration distributions.
The RMSD measure has a lower bound of 0 and higher values denote
higher deviations, but for a thorough explanation of the measure and the procedures
necessary for its computation, please refer to the original publication by
\cite{Andersson2017}. By computing the RMSD of \remodnav\ within the original set
of data from \citet{Andersson2017}, we are able to compare the similarity of the
three distribution parameters produced by \remodnav\ to human labeling and those of
the other algorithms, and thus rank \remodnav\ within the set of algorithms used in
\citet{Andersson2017}.
For a more straightforward interpretation of the RMSD measure, we converted
RMSDs per event and stimulus category into ranks (where 0 is the best).
The results of the RMSD computations are summarized in Tables \ref{tab:rmsd_fix},
\ref{tab:rmsd_sac}, \ref{tab:rmsd_pso} and \ref{tab:rmsd_pur}.
The "best" algorithm with regard to saccade and PSO detection in \cite{Andersson2017}
was the ``LNS" algorithm by \cite{Larsson2013}. \remodnav\ performs comparable to
``LNS" for saccades: Across stimulus categories, ``LNS" achieves a mean rank of $2.0$,
while \remodnav\ achieves a mean rank of $3.3$. For PSOs, across
stimulus categories, ``LNS" has a mean rank of $2.3$, while \remodnav\ achieves a
mean rank of $2.0$.
In the original set, different algorithms performed best for fixation detection
given the category. NH performed best for images  and videos, but worst for dots.
In our evaluation, \remodnav\ outperforms all other
algorithms in the dots category, and achieves rank 5 and 6 for videos and images,
respectively. That is, for fixation detection in videos and images, \remodnav\
performs in the middle range. Across all stimulus and event categories,
\remodnav\ achieves a median ranking of $3.0$ (both with and without inclusion of
its ranks for smooth pursuit detection). Thus, \remodnav\ detects the number
of events and their duration distribution consistently similar to human coders,
especially so for saccades and PSOs, and fixations in the dots category.


\begin{table*}[tbp]
  % table caption is above the table
  \caption{RMSD ranks of fixation parameters for various stimulation types}
  \label{tab:rmsd_fix}       % Give a unique label
  % For LaTeX tables use
  \begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}}lllllllllllll}
    \hline\noalign{\smallskip}
    & \multicolumn{4}{l}{Images} & \multicolumn{4}{l}{Dots} & \multicolumn{4}{l}{Videos}\\
    Algorithm & Mean & SD & \# & rank &  Mean & SD & \# & rank & Mean & SD & \# & rank \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    MN        & \FIXimgmnMN   & \FIXimgsdMN   & \FIXimgnoMN   & \rankFIXimgMN   &  \FIXdotsmnMN   & \FIXdotssdMN   & \FIXdotsnoMN   & \rankFIXdotsMN    & \FIXvideomnMN   & \FIXvideosdMN   & \FIXvideonoMN   & \rankFIXvideoMN    \\
    RA        & \FIXimgmnRA   & \FIXimgsdRA   & \FIXimgnoRA   & \rankFIXimgRA   &  \FIXdotsmnRA   & \FIXdotssdRA   & \FIXdotsnoRA   & \rankFIXdotsRA    & \FIXvideomnRA   & \FIXvideosdRA   & \FIXvideonoRA   & \rankFIXvideoRA    \\
    CDT       & \FIXimgmnCDT  & \FIXimgsdCDT  & \FIXimgnoCDT  & \rankFIXimgCDT  &  \FIXdotsmnCDT  & \FIXdotssdCDT  & \FIXdotsnoCDT  & \rankFIXdotsCDT   & \FIXvideomnCDT  & \FIXvideosdCDT  & \FIXvideonoCDT  & \rankFIXvideoCDT   \\
    EM        & -             & -             & -             & -               &  -              & -              & -              & -                 & -               & -               & -               & -                  \\
    IDT       & \FIXimgmnIDT  & \FIXimgsdIDT  & \FIXimgnoIDT  & \rankFIXimgIDT  &  \FIXdotsmnIDT  & \FIXdotssdIDT  & \FIXdotsnoIDT  & \rankFIXdotsIDT   & \FIXvideomnIDT  & \FIXvideosdIDT  & \FIXvideonoIDT  & \rankFIXvideoIDT   \\
    IKF       & \FIXimgmnIKF  & \FIXimgsdIKF  & \FIXimgnoIKF  & \rankFIXimgIKF  &  \FIXdotsmnIKF  & \FIXdotssdIKF  & \FIXdotsnoIKF  & \rankFIXdotsIKF   & \FIXvideomnIKF  & \FIXvideosdIKF  & \FIXvideonoIKF  & \rankFIXvideoIKF   \\
    IMST      & \FIXimgmnIMST & \FIXimgsdIMST & \FIXimgnoIMST & \rankFIXimgIMST &  \FIXdotsmnIMST & \FIXdotssdIMST & \FIXdotsnoIMST & \rankFIXdotsIMST  & \FIXvideomnIMST & \FIXvideosdIMST & \FIXvideonoIMST & \rankFIXvideoIMST  \\
    IHMM      & \FIXimgmnIHMM & \FIXimgsdIHMM & \FIXimgnoIHMM & \rankFIXimgIHMM &  \FIXdotsmnIHMM & \FIXdotssdIHMM & \FIXdotsnoIHMM & \rankFIXdotsIHMM  & \FIXvideomnIHMM & \FIXvideosdIHMM & \FIXvideonoIHMM & \rankFIXvideoIHMM  \\
    IVT       & \FIXimgmnIVT  & \FIXimgsdIVT  & \FIXimgnoIVT  & \rankFIXimgIVT  &  \FIXdotsmnIVT  & \FIXdotssdIVT  & \FIXdotsnoIVT  & \rankFIXdotsIVT   & \FIXvideomnIVT  & \FIXvideosdIVT  & \FIXvideonoIVT  & \rankFIXvideoIVT   \\
    NH        & \FIXimgmnNH   & \FIXimgsdNH   & \FIXimgnoNH   & \rankFIXimgNH   &  \FIXdotsmnNH   & \FIXdotssdNH   & \FIXdotsnoNH   & \rankFIXdotsNH    & \FIXvideomnNH   & \FIXvideosdNH   & \FIXvideonoNH   & \rankFIXvideoNH    \\
    BIT       & \FIXimgmnBIT  & \FIXimgsdBIT  & \FIXimgnoBIT  & \rankFIXimgBIT  &  \FIXdotsmnBIT  & \FIXdotssdBIT  & \FIXdotsnoBIT  & \rankFIXdotsBIT   & \FIXvideomnBIT  & \FIXvideosdBIT  & \FIXvideonoBIT  & \rankFIXvideoBIT   \\
    LNS       & -             & -             & -             &  -              &  -              & -              & -              &  -                & -               & -               & -               &  -                 \\
    \remodnav\ & \FIXimgmnRE   & \FIXimgsdRE   & \FIXimgnoRE   & \rankFIXimgRE   &  \FIXdotsmnRE   & \FIXdotssdRE   & \FIXdotsnoRE   & \rankFIXdotsRE    & \FIXvideomnRE   & \FIXvideosdRE   & \FIXvideonoRE   & \rankFIXvideoRE    \\
    \noalign{\smallskip}\hline
  \end{tabular*}

  \textit{Note}: Fixation distribution parameters for the algorithms
  reported in \citet{Andersson2017} and \remodnav\ (bottom row). RMSDs
  were converted into ranks (lower is better).

\end{table*}

\begin{table*}[tbp]
  % table caption is above the table
  \caption{RMSD ranks of saccade parameters for various stimulation types}
  \label{tab:rmsd_sac}       % Give a unique label
  % For LaTeX tables use
  \begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}}lllllllllllll}
    \hline\noalign{\smallskip}
    & \multicolumn{4}{l}{Images} & \multicolumn{4}{l}{Dots} & \multicolumn{4}{l}{Videos}\\
    Algorithm & Mean & SD & \# & rank &  Mean & SD & \# & rank & Mean & SD & \# & rank \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    MN        & \SACimgmnMN   & \SACimgsdMN   & \SACimgnoMN   & \rankSACimgMN   &  \SACdotsmnMN   & \SACdotssdMN   & \SACdotsnoMN   & \rankSACdotsMN    & \SACvideomnMN   & \SACvideosdMN   & \SACvideonoMN   & \rankSACvideoMN    \\
    RA        & \SACimgmnRA   & \SACimgsdRA   & \SACimgnoRA   & \rankSACimgRA   &  \SACdotsmnRA   & \SACdotssdRA   & \SACdotsnoRA   & \rankSACdotsRA    & \SACvideomnRA   & \SACvideosdRA   & \SACvideonoRA   & \rankSACvideoRA    \\
    CDT       & -             & -             & -             & -               &  -              & -              & -              & -                 & -               & -               & -               & -                  \\
    EM        & \SACimgmnEM   & \SACimgsdEM   & \SACimgnoEM   & \rankSACimgEM    &  \SACdotsmnEM   & \SACdotssdEM   & \SACdotsnoEM   & \rankSACdotsEM    & \SACvideomnEM   & \SACvideosdEM   & \SACvideonoEM   & \rankSACvideoEM    \\
    IDT       & \SACimgmnIDT  & \SACimgsdIDT  & \SACimgnoIDT  & \rankSACimgIDT  &  \SACdotsmnIDT  & \SACdotssdIDT  & \SACdotsnoIDT  & \rankSACdotsIDT   & \SACvideomnIDT  & \SACvideosdIDT  & \SACvideonoIDT  & \rankSACvideoIDT   \\
    IKF       & \SACimgmnIKF  & \SACimgsdIKF  & \SACimgnoIKF  & \rankSACimgIKF  &  \SACdotsmnIKF  & \SACdotssdIKF  & \SACdotsnoIKF  & \rankSACdotsIKF   & \SACvideomnIKF  & \SACvideosdIKF  & \SACvideonoIKF  & \rankSACvideoIKF   \\
    IMST      & \SACimgmnIMST & \SACimgsdIMST & \SACimgnoIMST & \rankSACimgIMST &  \SACdotsmnIMST & \SACdotssdIMST & \SACdotsnoIMST & \rankSACdotsIMST  & \SACvideomnIMST & \SACvideosdIMST & \SACvideonoIMST & \rankSACvideoIMST  \\
    IHMM      & \SACimgmnIHMM & \SACimgsdIHMM & \SACimgnoIHMM & \rankSACimgIHMM &  \SACdotsmnIHMM & \SACdotssdIHMM & \SACdotsnoIHMM & \rankSACdotsIHMM  & \SACvideomnIHMM & \SACvideosdIHMM & \SACvideonoIHMM & \rankSACvideoIHMM  \\
    IVT       & \SACimgmnIVT  & \SACimgsdIVT  & \SACimgnoIVT  & \rankSACimgIVT  &  \SACdotsmnIVT  & \SACdotssdIVT  & \SACdotsnoIVT  & \rankSACdotsIVT   & \SACvideomnIVT  & \SACvideosdIVT  & \SACvideonoIVT  & \rankSACvideoIVT   \\
    NH        & \SACimgmnNH   & \SACimgsdNH   & \SACimgnoNH   & \rankSACimgNH   &  \SACdotsmnNH   & \SACdotssdNH   & \SACdotsnoNH   & \rankSACdotsNH    & \SACvideomnNH   & \SACvideosdNH   & \SACvideonoNH   & \rankSACvideoNH    \\
    BIT       & -             & -             & -             & -               &  -              & -              & -              & -                 & -               & -               & -               & -                  \\
    LNS       & \SACimgmnLNS  & \SACimgsdLNS  & \SACimgnoLNS  & \rankSACimgLNS  &  \SACdotsmnLNS  & \SACdotssdLNS  & \SACdotsnoLNS  & \rankSACdotsLNS   & \SACvideomnLNS  & \SACvideosdLNS  & \SACvideonoLNS  & \rankSACvideoLNS   \\
    \remodnav\ & \SACimgmnRE   & \SACimgsdRE   & \SACimgnoRE   & \rankSACimgRE   &  \SACdotsmnRE   & \SACdotssdRE   & \SACdotsnoRE   & \rankSACdotsRE    & \SACvideomnRE   & \SACvideosdRE   & \SACvideonoRE   & \rankSACvideoRE    \\
    \noalign{\smallskip}\hline
  \end{tabular*}

  \textit{Note}: Saccade distribution parameters for the algorithms
  reported in \citet{Andersson2017} and \remodnav\ (bottom row). RMSDs
  were converted into ranks (lower is better).

\end{table*}

\begin{table*}[tbp]
  % table caption is above the table
  \caption{RMSD ranks of PSO parameters for various stimulation types}
  \label{tab:rmsd_pso}       % Give a unique label
  % For LaTeX tables use
  \begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}}lllllllllllll}
    \hline\noalign{\smallskip}
    & \multicolumn{4}{l}{Images} & \multicolumn{4}{l}{Dots} & \multicolumn{4}{l}{Videos}\\
    Algorithm & Mean & SD & \# & rank &  Mean & SD & \# & rank & Mean & SD & \# & rank \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    MN        & \PSOimgmnMN   & \PSOimgsdMN   & \PSOimgnoMN   & \rankPSOimgMN   &  \PSOdotsmnMN   & \PSOdotssdMN   & \PSOdotsnoMN   & \rankPSOdotsMN    & \PSOvideomnMN   & \PSOvideosdMN   & \PSOvideonoMN   & \rankPSOvideoMN    \\
    RA        & \PSOimgmnRA   & \PSOimgsdRA   & \PSOimgnoRA   & \rankPSOimgRA   &  \PSOdotsmnRA   & \PSOdotssdRA   & \PSOdotsnoRA   & \rankPSOdotsRA    & \PSOvideomnRA   & \PSOvideosdRA   & \PSOvideonoRA   & \rankPSOvideoRA    \\
    NH        & \PSOimgmnNH   & \PSOimgsdNH   & \PSOimgnoNH   & \rankPSOimgNH   &  \PSOdotsmnNH   & \PSOdotssdNH   & \PSOdotsnoNH   & \rankPSOdotsNH    & \PSOvideomnNH   & \PSOvideosdNH   & \PSOvideonoNH   & \rankPSOvideoNH    \\
    LNS       & \PSOimgmnLNS  & \PSOimgsdLNS  & \PSOimgnoLNS  & \rankPSOimgLNS  &  \PSOdotsmnLNS  & \PSOdotssdLNS  & \PSOdotsnoLNS  & \rankPSOdotsLNS   & \PSOvideomnLNS  & \PSOvideosdLNS  & \PSOvideonoLNS  & \rankPSOvideoLNS   \\
    \remodnav\ & \PSOimgmnRE   & \PSOimgsdRE   & \PSOimgnoRE   & \rankPSOimgRE   &  \PSOdotsmnRE   & \PSOdotssdRE   & \PSOdotsnoRE   & \rankPSOdotsRE    & \PSOvideomnRE   & \PSOvideosdRE   & \PSOvideonoRE   & \rankPSOvideoRE    \\
    \noalign{\smallskip}\hline
  \end{tabular*}

  \textit{Note}: PSO distribution parameters for the algorithms
  reported in \citet{Andersson2017} and \remodnav\ (bottom row). RMSDs
  were converted into ranks (lower is better).

\end{table*}

\begin{table*}[tbp]
  % table caption is above the table
  \caption{RMSD ranks of pursuit parameters for various stimulation types}
  \label{tab:rmsd_pur}       % Give a unique label
  % For LaTeX tables use
  \begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}}lllllllllllll}
    \hline\noalign{\smallskip}
    & \multicolumn{4}{l}{Images} & \multicolumn{4}{l}{Dots} & \multicolumn{4}{l}{Videos}\\
    Algorithm & Mean & SD & \# & rank &  Mean & SD & \# & rank & Mean & SD & \# & rank \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    MN        & \PURimgmnMN   & \PURimgsdMN   & \PURimgnoMN   & \rankPURimgMN   &  \PURdotsmnMN   & \PURdotssdMN   & \PURdotsnoMN   & \rankPURdotsMN    & \PURvideomnMN   & \PURvideosdMN   & \PURvideonoMN   & \rankPURvideoMN    \\
    RA        & \PURimgmnRA   & \PURimgsdRA   & \PURimgnoRA   & \rankPURimgRA   &  \PURdotsmnRA   & \PURdotssdRA   & \PURdotsnoRA   & \rankPURdotsRA    & \PURvideomnRA   & \PURvideosdRA   & \PURvideonoRA   & \rankPURvideoRA    \\
    \remodnav\ & \PURimgmnRE   & \PURimgsdRE   & \PURimgnoRE   & \rankPURimgRE   &  \PURdotsmnRE   & \PURdotssdRE   & \PURdotsnoRE   & \rankPURdotsRE    & \PURvideomnRE   & \PURvideosdRE   & \PURvideonoRE   & \rankPURvideoRE    \\
    \noalign{\smallskip}\hline
  \end{tabular*}

  \textit{Note}: Smooth pursuit distribution parameters for the algorithms
  reported in \citet{Andersson2017} and \remodnav\ (bottom row). RMSDs
  were converted into ranks (lower is better).

\end{table*}


The combined results of the analyses on the dataset used by
\citet{Andersson2017} demonstrate that \remodnav\ yields the highest congruency
to manual human annotation in any stimulus category on a sample-by-sample basis
compared to ten contemporary algorithms when detecting fixations, saccades,
and post-saccadic oscillations. Its performance stays good, and exceeds all other
algorithms performance in the dots and video category, when including smooth
pursuit detection as well. In this regard, the results reveal that REMoDNaVs
performance equals or exceeds those of the original NH. Taking the number of events
and their duration distribution into account, REMoDNaVs performance stays good.
Importantly \remodnav\ performs similar to human coders in almost all event
categories and stimulus types. NH outperforms it only for fixation detection
in the image and video category, however, in these categories \remodnav\ still
classifies comparatively well. Thus, \remodnav\ does improve the NH algorithm
in aspects that were  suboptimal, but does not diminish performance in NHs strong points.

%\subsubsection*{Evaluation of events}

\subsection*{Prolonged recordings of natural viewing}\label{ana_2}

% discuss inlab vs scanner data

% \todo[inline]{\textit{Need to determine exactly which plots and tables will
% go here}}

% \todo[inline]{\textit{This section contains the analysis of the main
% sequences -- which I assume is the meat of the thesis. The goal is to have as
% much information as possible be contained in figures and/or tables (and their
% captions). We don't pay for those, but we do pay for the main text body.}}

Given that \remodnav\ yields biologically plausible results for dynamic
stimulation as indicated by the previous results, we determined whether it is
capable of analyzing data from dynamic stimulation without a trial structure.
For this, we applied it to the high quality eye tracking dataset obtained from
15 subjects watching the movie in a laboratory setting. This analysis further
serves as a base line to compare results obtained from a lower quality
recording to. Eye movements in this high-quality dataset were recorded with an
Eyelink 1000 with a standard desktop mount (software version 4.51; SR Research
Ltd., Mississauga, Ontario, Canada) and a sampling rate of 1000Hz. The movie
stimulus was presented on a $522$x$294$mm LCD monitor at a resolution of $1920$
x $1280$px at a viewing distance of 85cm \citep{Hanke2016}.  The top panel in
\fig{remodnav} displays the performance of \remodnav\ for a high quality data
sample. An individual main sequence for a lab subject is displayed in the
bottom right panel in \fig{overallComp}. In order to compare event detection
between high and low quality sample, we computed the event distribution
parameters per event type and movie run. The top panel in \fig{dist}
displays the resulting distributions for the high quality dataset.

\begin{figure*}[tbp]
  % TODO make final figure and switch
  %\includegraphics[width=1\textwidth]{img/remodnav.eps}
  \includegraphics[trim=0 8mm 0 0,clip,width=1\textwidth]{img/remodnav_lab.pdf} \\
  \includegraphics[trim=0 0 0 3.5mm,clip,width=1\textwidth]{img/remodnav_mri.pdf}\\
  \caption{Exemplary classification performance of \remodnav\ in a high quality lab sample (top panel) and low
  quality MRI sample (bottom panel), each for the same 10 seconds of movie stimulus.
  Black lines depict x and y coordinates of eye gaze, gray graph represents velocities. Colors indicate labeled
  eye events. Beige: Fixation,
  light green: Saccade, brown: Smooth pursuit, dark blue: High velocity PSOs, light blue: Low velocity PSOs.
  White: no signal.}

  \label{fig:remodnav}
\end{figure*}

\begin{figure*}
  % Use the relevant command to insert your figure file.
  \includegraphics[width=0.24\textwidth]{img/hist_fixation_lab.pdf}
  \includegraphics[width=0.24\textwidth]{img/hist_saccade_lab.pdf}
  \includegraphics[width=0.24\textwidth]{img/hist_PSO_lab.pdf}
  \includegraphics[width=0.24\textwidth]{img/hist_pursuit_lab.pdf} \\
  \includegraphics[width=0.24\textwidth]{img/hist_fixation_mri.pdf}
  \includegraphics[width=0.24\textwidth]{img/hist_saccade_mri.pdf}
  \includegraphics[width=0.24\textwidth]{img/hist_PSO_mri.pdf}
  \includegraphics[width=0.24\textwidth]{img/hist_pursuit_mri.pdf} \\
  % figure caption is below the figure
  \caption{Comparison of eye movement event duration distributions for the
    high-quality lab sample (top row), and the low quality MRI sample
    (bottom row) across all participants (each $N=15$), and the entire duration of
    the same feature-length movie stimulus. All histograms depict absolute
    number of events. Visible differences are limited to an overall lower number of
    events, and fewer long saccades for the MRI sample. These are attributable
    to a higher noise level and more signal loss \citep[compare][Fig.
    4b]{Hanke2016} in the MRI sample, and to stimulus size differences
    (\unit[23.75]{\textdegree} MRI vs. \unit[34]{\textdegree} lab).}
  \label{fig:dist}
  % Give a unique label
\end{figure*}

%\begin{figure*}[tbp]
%  \includegraphics[width=1\textwidth]{img/main_sequences.eps}
%  \caption{Main sequences for four subjects, during one 15 minute sequence of
%  the movie. Top panel: Lab subjects, bottom panel: MRI subjects.}
%  \label{fig:mains}
%\end{figure*}

\subsection*{Low-quality data}\label{ana_3}

In order to determine the robustness of \remodnav\ to noise, in a last step, we
applied it to the low quality eye tracking dataset of the studyforrest
publication. This data was recorded with a different Eyelink 1000 (software
version 4.594) equipped with an MR-compatible telephoto lens and illumination
kit (SR Research Ltd., Mississauga, Ontario, Canada) at $1000$Hz during
simultaneous fMRI acquisition. The movie was presented at a viewing distance of
$63$cm on a 26cm ($1280$ x $1024$px) LCD screen in 720p resolution at full
width. The eye tracking camera was mounted outside the scanner bore and
recorded the participants left eye at a distance of about 100cm
\citep{Hanke2016}.  The lower data quality in this dataset is evident from a
generally higher amount of data loss and a larger spatial uncertainty (see
\citet{Hanke2016} for details on data quality). As the stimulus material was
the same between the high and low quality data sample, \remodnav\ demonstrates
robust performance if the characteristics of extracted events within the low
dataset correspond to those obtained from the previous high quality data
sample.

% N+H describe: "Data quality is related to accuracy, precision, percentage of
% data loss, perhaps in addition to a subjective rating from the person
% responsible for the recording"

\fig{remodnav} displays the performance of \remodnav\ for a low
quality data sample in the bottom row.  As evident in both panels, periods with
data loss do not interfere with the detection of eye movement events in their
vicinity beyond the border region of data loss that is masked during
preprocessing. The detection of saccades corresponds to the velocity peaks in
the time series. An individual main sequence for an MRI subject is displayed
in the top right panel in \fig{overallComp}, and the left column in
\fig{overallComp} shows all events produced from both datasets. Lastly,
\fig{dist} displays the distribution parameters per event type for the low
quality dataset in the bottom panel.
All together, \fig{remodnav}, \fig{dist} and
\fig{overallComp} illustrate a high similiarity between performance in high
and low quality datasets. In both cases, the main sequences follow the known
peak-velocity -- amplitude relationship of saccades. It is noteable that the
MRI sample displays fewer saccades of the shortest range. Given the higher
spatial uncertainty in this dataset, this is unsurprising: the eye tracker
likely was not able to capture these small eye movements under the low-light
conditions. Apart from this aspect, both main sequences are comparable. The
resulting duration distributions between both datasets are similar, despite the
generally lower amount of events in the low quality dataset. Given
that \remodnav\ classifies data from dynamic stimulation well, and that results
on low and high quality data yield similar biologically plausible results, the
performance seems to be robust even under high noise conditions.

\begin{figure*}[tbp]
  \includegraphics[width=.5\textwidth]{img/mainseq_mri_300dpi.png}
  \includegraphics[width=.5\textwidth]{img/mainseq_sub_mri_300dpi.png} \\
  \includegraphics[width=.5\textwidth]{img/mainseq_lab_300dpi.png}
  \includegraphics[width=.5\textwidth]{img/mainseq_sub_lab_300dpi.png} \\

  \caption{Main sequence of eye movement events during one 15 minute sequence of
  the movie (segment 2) for MRI (top), and lab participants (bottom). Data
  across all participants per dataset is shown on the left, and data for a single
  exemplary participant on the right.}

  \label{fig:overallComp}
\end{figure*}


% move to discussion?
The literature on eye event detection algorithms suggests that there is no true
``one-fits-all" solution for event detection.  There are some successful
approaches using deep neural networks \citep{Startsev2018}, but those need
appropriately large amounts of suitable training data, that may not be easily
available.
% till here
As evident from many evaluations and applications of algorithms (e.g.
\cite{Andersson2017}, \cite{Larsson2013}, \cite{Zemblys2018}, \cite{5523936}),
different underlying stimulation, data characteristics, or use cases make
certain algorithms more suitable than others, hard-coded thresholds and
parameters either applicable or detrimental, or potentially require changes to
the default values or parameters. For our own analyses, the default parameter
values were well applicable. But we cannot anticipate how the data from other
groups or paradigms looks like.  \remodnav\ therefore contains a broad range of
transparent, interpretable and adjustable parameters with default values
justified from physiological findings or biological limitations, and no
hard-coded values. This gives users an option to adjust the algorithm to their
data's characteristics, but offers a sensible starting point with defaults that
proved to work well with the different datasets we evaluated the algorithm on.
Only velocity thresholds for saccade detection are estimated iteratively from
the data. Apart from the advantage of taking noise levels into account, this
also follows the reasoning of \cite{Nystrom2010AnData} that uninformed
adjustments of this parameter can have vast consequences on saccade detection,
and, subsequently, the labeling of all other eye events.


\section*{Conclusion}\label{con}

Based on the adaptive, velocity-based algorithm for fixation, saccade, and
glissade detection by \cite{Nystrom2010AnData}, we have developed a novel,
robust algorithm for the classification of various eye movements in eye
tracking data from dynamic stimulation.  The combined results from all analyses
are encouraging. Based on data from dynamic and static stimulation from the
Andersson dataset, \remodnav\ performs well and even outperforms contemporary
algorithms, and this is especially true for dynamic stimulation. Therefore, we
are confident that \remodnav\ is currently a good choice for event detection
from eye gaze data obtained during dynamic stimulation.  Furthermore, the
algorithm yields similar, and thus robust, results for high and low quality
data. We specifically developed \remodnav\ to yield robust results with dynamic
stimulation, in particular noisy data such as the eye tracking data recorded
simultaneously to fMRI acquisition. These results confirm that \remodnav\ is
well able to detect eye movements from dynamic stimulation, even when the
quality of data is low. Therefore, the algorithm is a promising tool for
research paradigms with dynamic stimuli where lower data quality is an inherent
property of the acquisition procedure, such as simultaneous fMRI and eye gaze
recordings, or usage of remote eye trackers.  Furthermore, the algorithm proved
to be well applicable to other types of stimulation. Its classification
performance on static images was good. This demonstrates that our changes to
the original NH algorithm did not interfere with its performance on its strong
points. Lastly, \remodnav\ is a user-friendly tool with options for to customize
it to individual use cases. It is OS independent, uses only freely available
software, is easy installable, provides simple command-line execution, and is
ready-to-use without manual labeling or other forms of training data being
required. As such, REMoDNaV is a versatile tool that provides researchers with
a robust method to classify eye movements obtained from different types of
stimulation.

Just as \cite{Andersson2017}, we considered human annotations to be a gold
standard in event detection when we compared different algorithms performances.
The implications of the obtained results from this comparison are hence only
valid if this assumption is warranted. Some authors voice concern about human
annotation, e.g. \cite{5523936}, as for the potential of biases that limit
generalizability. However, \cite{Hooge2018} concluded that - despite obvious
variability between human annotations - manual labeling is certainly important
at least in algorithm validation as we undertook it in our study. Continuing in
the same vein, the \remodnav\ algorithm is - as all event detection algorithms -
far from perfect. Despite superior performance during dynamic stimulation
compared to other algorithms, the results do not mirror human annotations
exactly. Therefore, as with any other algorithm, we recommend that every user
should at least eye-ball their results for plausibility. One straightforward
way to do this is to study the classification performance as shown in
\fig{remodnav}. The \remodnav\ module will provide such a plot automatically
for every detection task, enabling researchers to detect implausible results
early on

\todo[inline]{There are a number of (some very recent) ML approaches
  (Startsev,2018; Komogortsev,2010) that perform well. We should acknowledge
  that but also find arguments to justify our approach. One advantage of
  \remodnav\ is how accessible and readily usable it is. To underline this, I
  propose we update the README.md on Github similarly to
  https://github.com/MikhailStartsev/deep\_em\_classifier in that we provide a
  use case/tutorial: what is the input/how should it look like, what is the
  command line call, how does the output look like. I think it is a great
  advantage of \remodnav\ that we work with more "typical" file formats. Also,
  \remodnav\ does not need training, and hence no extensive labeled data as
  training sets (that could be tedious to create, or not fitting to exact use
  case if taken from e.g. open datasets)}


% \section*{Notes} % Optional - only if NO new datasets are included

% This section is required if the paper does not include novel data or
% analyses.  It allows authors %to briefly summarize the key points from the
% article.

%% we introduce no new data

% \section*{Data availability} % Optional - only if novel data or analyses are
% included

% Please add details of where any datasets that are mentioned in the paper, and
% that have not have not previously been formally published, can be found.  If
% previously published datasets are mentioned, these should be cited in the
% references, as per usual scholarly conventions.

% \todo[inline]{This will be completed at the end with info on where readers
% can obtain data and code}


\section*{Software availability}

The latest version of \remodnav\ can be installed from PyPi via \texttt{pip
install remodnav}. It is recommended to use a dedicated virtualenv. The source
code of the software can be found on Github \\
(https://github.com/psychoinformatics-de/remodnav). All bugs, concerns and
enhancement requests for this software should be submitted via Github.
Questions with regard to REMoDNaV can be submitted to NeuroStars.org with a
\texttt{remodnav} tag. NeuroStars.org is a platform similar to StackOverflow
but dedicated to neuroinformatics.  \remodnav\ is released under MIT license.


% This section will be generated by the Editorial Office before publication.
% Authors are asked to provide some initial information to assist the Editorial
% Office, as detailed below.

%\begin{enumerate}
%\item URL link to where the software can be downloaded from or used by a non-coder (AUTHOR TO PROVIDE; optional)
%\item URL link to the author's version control system repository containing the source code (AUTHOR TO PROVIDE; required)
%\item Link to source code as at time of publication ({\textit{F1000Research}} TO GENERATE)
%\item Link to archived source code as at time of publication ({\textit{F1000Research}} TO GENERATE)
%\item Software license (AUTHOR TO PROVIDE; required)
%\end{enumerate}


\subsection*{Author contributions}

% In order to give appropriate credit to each author of an article, the individual
% contributions of each author to the manuscript should be detailed in this section. We
% recommend using author initials and then stating briefly how they contributed.

AD, MH conceived and implemented the algorithm.
AD, AW, MH validated algorithm performance.
AD, AW, MH wrote the manuscript.

\subsection*{Competing interests}

% All financial, personal, or professional competing interests for any of the authors that
% could be construed to unduly influence the content of the article must be disclosed and
% will be displayed alongside the article.

No competing interests were disclosed.

\subsection*{Grant information}

% Please state who funded the work discussed in this article, whether it is your employer,
% a grant funder etc. Please do not list funding that you have that is not relevant to this
% specific piece of research. For each funder, please state the funder’s name, the grant
% number where applicable, and the individual to whom the grant was assigned.
% If your work was not funded by any grants, please include the line: ‘The author(s)
% declared that no grants were involved in supporting this work.’

Michael Hanke was supported by funds from the German federal state of
Saxony-Anhalt and the European Regional Development Fund (ERDF),
Project: Center for Behavioral Brain Sciences (CBBS).
Adina Wagner was supported by the German Academic Foundation.

\textit{The funders had no role in study design, data collection and analysis,
decision to publish, or preparation of the manuscript.}

% \todo[inline]{the journals template.tex says to use bibitem to create
% references. we should do that once finished here}

\begin{acknowledgements}

% This section should acknowledge anyone who contributed to the research or the
% article but who does not qualify as an author based on the criteria provided earlier
% (e.g. someone or an organisation that provided writing assistance). Please state how
% they contributed; authors should obtain permission to acknowledge from all those
% mentioned in the Acknowledgements section.
% Please do not list grant funding in this section.

This work is based on an earlier Python implementation and evaluation of the
original NH algorithm by Ulrike~Schnaithmann and Isabel~Dombrowe.  We are
grateful to \cite{Andersson2017} for releasing the labeled eye tracking dataset
used for validation under an open-source license.

\end{acknowledgements}

\bibliographystyle{spbasic}      % basic style, author-year citations
\bibliography{EyeGaze,tools,references}

% References can be listed in any standard referencing style that uses a numbering system
% (i.e. not Harvard referencing style), and should be consistent between references within
% a given article.

% Reference management systems such as Zotero provide options for exporting
% bibliographies as Bib\TeX{} files. Bib\TeX{} is a bibliographic tool that is
% used with \LaTeX{} to help organize the user's references and create a
% bibliography. This template contains an example of such a file,
% \texttt{sample.bib}, which can be replaced with your own. Use the
% \verb|\cite| command  to create in-text citations, like this
% \cite{Smith:2012qr} and this \cite{Smith:2013jd}.


% See this guide for more information on BibTeX:
% http://libguides.mit.edu/content.php?pid=55482&sid=406343

% For more author guidance please see:
% http://f1000research.com/author-guidelines

% When all authors are happy with the paper, use the
% ‘Submit to F1000Research' button from the menu above
% to submit directly to the open life science journal F1000Research.

% Please note that this template results in a draft pre-submission PDF document.
% Articles will be professionally typeset when accepted for publication.

% We hope you find the F1000Research Overleaf template useful,
% please let us know if you have any feedback using the help menu above.


\clearpage
\listoftodos

\end{document}
